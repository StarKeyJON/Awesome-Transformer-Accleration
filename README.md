## GPT

* [Quadapter: Adapter for GPT-2 Quantization](https://arxiv.org/pdf/2211.16912.pdf) - *Minseop Park et al,* `arxiv 2022`

* [GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS](https://arxiv.org/pdf/2210.17323.pdf) - *Elias Frantar et al,* `arxiv 2022`

* [Compression of Generative Pre-trained Language Models via Quantization](https://arxiv.org/pdf/2203.10705.pdf) - *Chao et al*, `ACL 2022`

* [NUQMM: QUANTIZED MATMUL FOR EFFICIENT INFERENCE OF
  LARGE-SCALE GENERATIVE LANGUAGE MODELS](https://arxiv.org/pdf/2206.09557.pdf) - *Park et al*, `arxiv 2022`

## BERT

* [Q8BERT: Quantized 8Bit BERT](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9463531) - *Zafrir et al*, `NIPS Workshop 2019`

* [ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](https://arxiv.org/pdf/2206.01861.pdf) - *Yao et al*, `arxiv 2022`

