## ViT

* [Post-Training Quantization for Vision Transformer](https://arxiv.org/abs/2106.14156) - *PKU & Huawei Noah’s Ark Lab*, `NIPS 2021`

* [PTQ4ViT: Post-Training Quantization Framework for Vision Transformers](https://arxiv.org/pdf/2111.12293v2) - *Houmo AI & PKU*, `ECCV 2021`

* [FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer](https://arxiv.org/pdf/2111.13824) - *MEGVII Technology*, `IJCAI 2022`

* [Q-ViT: Fully Differentiable Quantization for Vision Transformer](https://arxiv.org/pdf/2201.07703) - *Megvii Technology & CASIA*, `arxiv 2022`

* [TerViT: An Efficient Ternary Vision Transformer](https://arxiv.org/pdf/2201.08050v2) - *Beihang University & Shanghai Artificial Intelligence Laboratory*, `arxiv 2022`

* [Patch Similarity Aware Data-Free Quantization for Vision Transformers](https://arxiv.org/abs/2203.02250) - *CASIA*, `ECCV 2022`

* [PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers](https://arxiv.org/abs/2209.05687) - *CASIA*, `arxiv 2022`

## BERT

* [Q8BERT: Quantized 8Bit BERT](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9463531) - *Intel AI Lab*, `NIPS Workshop 2019`

* [Ternarybert: Distillation-aware ultra-low bit bert](https://arxiv.org/pdf/2009.12812.pdf) - *Huawei Noah’s Ark Lab*, `EMNLP 2020`

* [I-BERT: Integer-only BERT Quantization](https://arxiv.org/pdf/2101.01321v3.pdf) - *University of California, Berkeley*, `ICML 2021`

* [Understanding and Overcoming the Challenges of Efficient Transformer Quantization](https://aclanthology.org/2021.emnlp-main.627) - *Qualcomm AI Research*, `EMNLP 2021`

* [ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](https://arxiv.org/pdf/2206.01861.pdf) - *Microsoft*, `arxiv 2022`

* [Outlier Suppression: Pushing the Limit of Low-bit Transformer](https://arxiv.org/abs/2209.13325) - *BUAA & SenseTime & PKU & UESTC*, `NIPS 2022`

## GPT

* [Compression of Generative Pre-trained Language Models via Quantization](https://arxiv.org/pdf/2203.10705.pdf) - *The University of Hong Kong & Huawei Noah’s Ark Lab*, `ACL 2022`

* [NUQMM: QUANTIZED MATMUL FOR EFFICIENT INFERENCE OF
  LARGE-SCALE GENERATIVE LANGUAGE MODELS](https://arxiv.org/pdf/2206.09557.pdf) - *Pohang University of Science and Technology*, `arxiv 2022`

* [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339) - *University of Washington & FAIR*, `NIPS 2022`
  
* [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](http://arxiv.org/abs/2211.10438) - *MIT*, `arxiv 2022`

* [GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS](https://arxiv.org/pdf/2210.17323.pdf) - *IST Austria & ETH Zurich*,  `arxiv 2022`

* [Quadapter: Adapter for GPT-2 Quantization](https://arxiv.org/pdf/2211.16912.pdf) - *Qualcomm AI Research*,  `arxiv 2022`

* [SPARSEGPT: MASSIVE LANGUAGE MODELS CAN BE ACCURATELY PRUNED IN ONE-SHOT](https://arxiv.org/pdf/2301.00774.pdf) - *IST Austria*, `arxiv 2023`

updating ...
