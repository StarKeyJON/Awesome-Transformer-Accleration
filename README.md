## GPT

* [Quadapter: Adapter for GPT-2 Quantization](https://arxiv.org/pdf/2211.16912.pdf) - *Qualcomm AI Research,* `arxiv 2022`

* [GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS](https://arxiv.org/pdf/2210.17323.pdf) - *IST Austria & ETH Zurich,* `arxiv 2022`

* [Compression of Generative Pre-trained Language Models via Quantization](https://arxiv.org/pdf/2203.10705.pdf) - *The University of Hong Kong & Huawei Noah’s Ark Lab*, `ACL 2022`

* [NUQMM: QUANTIZED MATMUL FOR EFFICIENT INFERENCE OF
  LARGE-SCALE GENERATIVE LANGUAGE MODELS](https://arxiv.org/pdf/2206.09557.pdf) - *Pohang University of Science and Technology*, `arxiv 2022`
  
* [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](http://arxiv.org/abs/2211.10438) - *MIT*, `arxiv 2022`


## BERT

* [Understanding and Overcoming the Challenges of Efficient Transformer Quantization](https://aclanthology.org/2021.emnlp-main.627) - *Qualcomm AI Research*, `EMNLP 2021`

* [Ternarybert: Distillation-aware ultra-low bit bert](https://arxiv.org/pdf/2009.12812.pdf) - *Huawei Noah’s Ark Lab*, `EMNLP 2020`

* [Q8BERT: Quantized 8Bit BERT](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9463531) - *Intel AI Lab*, `NIPS Workshop 2019`

* [ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](https://arxiv.org/pdf/2206.01861.pdf) - *Microsoft*, `arxiv 2022`

* [I-BERT: Integer-only BERT Quantization](https://arxiv.org/pdf/2101.01321v3.pdf) - *University of California, Berkeley*, `ICML 2021`

